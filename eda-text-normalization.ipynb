{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/text-normalization-challenge-english-language/en_train.csv.zip')\ntest1 = pd.read_csv('../input/text-normalization-challenge-english-language/en_test.csv.zip')\ntest2 = pd.read_csv('../input/text-normalization-challenge-english-language/en_test_2.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('test_1 data shape',test2.shape)\ntest1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('test_2 data  shape',test2.shape)\ntest2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_list = [test1,test2]\ntest = pd.concat(test_list)\nprint('test data  shape',test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train data shape',train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Each sentence and each oken has an id\n* Each token has a class to show its type\n* the main goal of the project is to transorm tokens of the column \"before to those of the column \"after\" "},{"metadata":{"trusted":true},"cell_type":"code","source":"(test.shape[0])/(test.shape[0]+train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NB: the test size = 0.17 (roughly equal to 0.2)"},{"metadata":{},"cell_type":"markdown","source":"***let's take a look at the submission dataset***"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/text-normalization-challenge-english-language/en_sample_submission.csv.zip\")\nsubmission.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The submission dataset has an id that contains the sentence and the token id as sentence_token combination. "},{"metadata":{},"cell_type":"markdown","source":" ***the Number of sentences***"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the number of sentences in the train dataset: ',len(train.sentence_id.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the number of sentences in the test dataset: ',len(test.sentence_id.unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***the Number of tokens***"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sentences =train.groupby('sentence_id')[\"sentence_id\"].count()\nprint('the number of tokens in each sentence')\nprint(train_sentences.head())\nprint('--------------------------------------------------------------------------')\nprint(train_sentences.describe())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(20,10))\nsns.set_style(\"whitegrid\")\ncount_length_fig = sns.countplot(train_sentences , ax=ax)\nfor item in count_length_fig.get_xticklabels():\n    item.set_rotation(90)\n \nplt.title('tokens in each sentenes in the train set')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test\ntest_sentences =test.groupby('sentence_id')[\"sentence_id\"].count()\nprint('the number of tokens in each sentence')\nprint(test_sentences.head())\nprint('--------------------------------------------------------------------------')\nprint(test_sentences.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean and quantiles are shifted a bit to higher values."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(20,10))\nsns.set_style(\"whitegrid\")\ncount_length_fig = sns.countplot(train_sentences , ax=ax)\nfor item in count_length_fig.get_xticklabels():\n    item.set_rotation(90)\nplt.title('tokens in each sentenes in the test set')  \nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice a peak for sentences that are 7 tokens long in both train and test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#the longest train sentence\nmax_id = train_sentences[train_sentences == train_sentences.max()].index.values\nlong_example = train[train.sentence_id==max_id[0]].before.values.tolist()\nlong_example = ' '.join(long_example)\nprint('id={}: {}'.format(max_id,long_example))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like a literature reference."},{"metadata":{"trusted":true},"cell_type":"code","source":"#some samllest sentence\nmin_id = train_sentences[train_sentences == train_sentences.min()].index.values\nfor n in range(5):\n    small_example = train[train.sentence_id==min_id[n]].before.values.tolist()\n    small_example= ' '.join(small_example)\n    print('id={}: {}'.format(min_id[n],small_example))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dates"},{"metadata":{"trusted":true},"cell_type":"code","source":"# some of the median sentences\nmedian_id = train_sentences[train_sentences == train_sentences.median()].index.values\nfor n in range(5):\n    median_example = train[train.sentence_id==median_id[n]].before.values.tolist()\n    median_example= ' '.join(median_example)\n    print('id={}: {}'.format(median_id[n],median_example))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ***Tokens***"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train.token_id.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"after reading the explanations for the data in detail: Each token within a sentence has a token_id. Consequently the longest sentence has token ids ranging from 0 to 255 (256), and one of the smallest from 0 to 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('we have {} token class. '.format(len(train['class'].unique())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train['class'].value_counts()\ntrain.groupby(\"class\")[\"class\"].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(10,12))\n#sns.set_style(\"whitegrid\")\ncount_classes_fig = sns.countplot(y=\"class\", data=train, ax=ax)\nfor item in count_classes_fig.get_xticklabels():\n    item.set_rotation(45)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of classes are plain. But there are also some exotic classes."},{"metadata":{},"cell_type":"markdown","source":"Understanding of the **ELECTRONIC** and **VERBATIM** classes."},{"metadata":{},"cell_type":"markdown","source":"**ELECTRONIC**"},{"metadata":{"trusted":true},"cell_type":"code","source":"most_electronic_cases = train[train[\"class\"]=='ELECTRONIC'].groupby(\"before\")[\"before\"].count(\n).sort_values(ascending=False).head()\nsns.barplot(most_electronic_cases.index, most_electronic_cases.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**VERBATIM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"most_verbatim_cases = train[train[\"class\"]=='VERBATIM'].groupby(\"before\")[\"before\"].count(\n).sort_values(ascending=False).head(15)\n\nsns.barplot(most_verbatim_cases.index, most_verbatim_cases.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Most of Before words**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('unique before words in train: ',len(train.before.unique()))\nprint('unique before words in test: ',len(test.before.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train train\ntrain_word_counts = train.groupby(\"before\")[\"before\"].count().sort_values(ascending=False).head(15)\n\nsns.barplot(x=train_word_counts.index, y=train_word_counts.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test set\ntest_word_counts = test.groupby(\"before\")[\"before\"].count().sort_values(ascending=False).head(15)\n\nsns.barplot(x=test_word_counts.index, y=test_word_counts.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Pattern of changes**"},{"metadata":{},"cell_type":"markdown","source":"The before tokens changed after normalization in the train set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"change\"] = 0\ntrain.loc[train.before!=train.after, \"change\"] = 1\ntrain[\"change\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The tokens changed most often."},{"metadata":{"trusted":true},"cell_type":"code","source":"most_changed_words = train[train.change==1].groupby(\"before\")[\"before\"].count(\n).sort_values(ascending=False).head(20)\n\nsns.barplot(x=most_changed_words.index, y=most_changed_words.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To which class do the changed words belong most often?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"changes_classes_fig = sns.countplot(x=\"class\", data=train[train.change==1])\nfor item in changes_classes_fig.get_xticklabels():\n    item.set_rotation(45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**overlapping class contents**"},{"metadata":{},"cell_type":"markdown","source":"Actually I find it a bit strange that we have a digit class and a decimal, date, cardinal, ordinal, fraction class etc.. Are the tokens in these classes really all of a different kind or can we find the same token for example in date AND digit? And if so, why are they classified this or that way (does this depend on the sentence content?)."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_digits = set(train[train[\"class\"]==\"DIGIT\"].before.unique().tolist())\n\nunique_dates = set(train[train[\"class\"]==\"DATE\"].before.unique().tolist())\n#we can see other columns like DECIMAl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"overlap = unique_digits.intersection(unique_dates)\nlen(overlap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes, as we can see by this exampke: We have overlapping class contents!!\n\nThis could be good or bad. On the one hand it could depend on the sentence content to which class a token was assigned to. Then the class would also contain this context information implicitly. But on the other hand what if this is not true and the class assignment is somehow dirty or not finetuned enough? Then this opens the door for class feature engineering."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}